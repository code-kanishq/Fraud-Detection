"""Fraud Detection Model in Python

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BniVFxM42zuq98Eu_9rQaef0ys6vmMbO

**Importing Libraries**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    roc_auc_score,
    roc_curve,
    precision_recall_curve,
    average_precision_score
)
import xgboost as xgb # A powerful and popular gradient boosting library. I am using this model because of high accuracy and best possible predictive performance.

# Set some visualization styles
sns.set_style('whitegrid')
plt.rcParams['figure.figsize'] = (12, 8)

"""**Mounting to google drive to excess the dataset. I have uploaded the dataset on google drive the the path is given below. I am using Google Colab instead of Jupyter Notebook because of easy access to the dataset.**"""

from google.colab import drive
drive.mount('/content/drive')
print("\nGoogle Drive mounted successfully!")

"""**Loading the data from my Google Drive**


"""

try:
    file_path = '/content/drive/MyDrive/Internships/Accredian/Dataset/Fraud.csv'

    df = pd.read_csv(file_path)
    print("\nDataset loaded successfully from Google Drive!")
    print("Shape of the dataset:", df.shape)
    print("\nFirst 5 rows of the dataset:")
    print(df.head())
except FileNotFoundError:
    print(f"\nError: The file was not found at the path: {file_path}")
    print("Please make sure the file path is correct.")
    # A dummy dataframe is created as a fallback for demonstration
    data = {
        'step': np.arange(1, 101),
        'type': np.random.choice(['PAYMENT', 'TRANSFER', 'CASH_OUT', 'DEBIT', 'CASH_IN'], 100),
        'amount': np.random.uniform(10, 500000, 100),
        'nameOrig': [f'C{i}' for i in range(100)],
        'oldbalanceOrg': np.random.uniform(0, 1000000, 100),
        'newbalanceOrig': np.random.uniform(0, 1000000, 100),
        'nameDest': [f'M{i}' for i in range(100)],
        'oldbalanceDest': np.random.uniform(0, 2000000, 100),
        'newbalanceDest': np.random.uniform(0, 2000000, 100),
        'isFraud': np.random.choice([0, 1], 100, p=[0.98, 0.02]),
        'isFlaggedFraud': np.zeros(100)
    }
    df = pd.DataFrame(data)
    print("\nNOTE: A dummy dataset has been created for demonstration purposes.")
    df = None # The df is set to None to prevent rest of the script from running with dummy data

"""**Explratory Data Analysis (EDA)**



1.   Missing Values



"""

if df is not None:
    print("\n--- Exploratory Data Analysis ---")
    print("\nDataset Information:")
    df.info()

    print()
    print("\nMissing Values Check:")
    print(df.isnull().sum())

"""2. Descriptive Statistics"""

print()
    print("\nDescriptive Statistics:")
    print(df.describe())

"""3. The distribution of the target variable 'isFraud'"""

print("\nFraud vs. Non-Fraud Transactions:")
    fraud_counts = df['isFraud'].value_counts()
    print(fraud_counts)

"""4. Visualization of the class distribution to highlight the class imbalance"""

plt.figure(figsize=(6, 4))
    sns.countplot(x='isFraud', data=df)
    plt.title('Distribution of Fraudulent Transactions')
    plt.xlabel('Is Fraud (0: No, 1: Yes)')
    plt.ylabel('Number of Transactions')
    plt.show()

"""5. Exploring transaction types related to fraud"""

fraud_by_type = df.groupby('type')['isFraud'].sum()
    print("\nNumber of fraudulent transactions by type:")
    print(fraud_by_type)

    plt.figure(figsize=(10, 6))
    fraud_by_type.plot(kind='bar')
    plt.title('Fraudulent Transactions by Type')
    plt.xlabel('Transaction Type')
    plt.ylabel('Number of Fraudulent Transactions')
    plt.xticks(rotation=45)
    plt.show()
    # This confirms that 'TRANSFER' and 'CASH_OUT' are the types where fraud occurs.

"""**Data Preprocessing & Feature Engineering**

1. Here we will prepare the data for the machine learning model.
"""

if df is not None:
    print("\n--- Data Preprocessing & Feature Engineering ---")
    # Based on EDA, it is observed that fraud only occurs in 'TRANSFER' and 'CASH_OUT' types.
    df_filtered = df[df['type'].isin(['TRANSFER', 'CASH_OUT'])].copy()
    print(f"Original dataset size: {df.shape[0]}")
    print(f"Filtered dataset size: {df_filtered.shape[0]}")

"""2. Creating time-based features from 'step'"""

df_filtered['hour_of_day'] = df_filtered['step'] % 24
    df_filtered['day_of_month'] = (df_filtered['step'] // 24) % 30 # Here we are assuming a simulation of 30 days

"""3. Creating features for balance errors"""

df_filtered['errorBalanceOrig'] = df_filtered['newbalanceOrig'] + df_filtered['amount'] - df_filtered['oldbalanceOrg']
    df_filtered['errorBalanceDest'] = df_filtered['oldbalanceDest'] + df_filtered['amount'] - df_filtered['newbalanceDest']

"""4. Creating a feature indicating if the origin account was emptie because this is a strong indicator based on the data dictionary description of fraud."""

df_filtered['emptiedOrigAcct'] = ((df_filtered['oldbalanceOrg'] - df_filtered['amount']) == 0).astype(int)

"""5. Drop 'isFlaggedFraud' as it's a simple rule and could cause data leakage,  and also drop nameOrig and nameDest as they both are identifiers."""

X = df_filtered.drop(['isFraud', 'isFlaggedFraud', 'nameOrig', 'nameDest'], axis=1) # Define features (X)
    y = df_filtered['isFraud']  #Define target (y)

"""6. Identifying categorical and numerical features"""

categorical_features = ['type']
    numerical_features = X.select_dtypes(include=np.number).columns.tolist()

"""7. Creating a preprocessing pipeline"""

# Here one-hot encoding is used for categorical data and scaling is used for numerical data.
preprocessor = ColumnTransformer(
        transformers=[
            ('num', StandardScaler(), numerical_features),
            ('cat', OneHotEncoder(), categorical_features)
        ],
        remainder='passthrough'
)

"""8. Spliting the data into training and testing sets"""

X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
 )

print("\nData has been preprocessed and split into training and testing sets.")
print("Training set shape:", X_train.shape)
print("Testing set shape:", X_test.shape)

"""**Model Training**

Training the XGBoost Classifier.
"""

if 'X_train' in locals() and df is not None:
    print("\n--- Model Training ---")
    scale_pos_weight = y_train.value_counts()[0] / y_train.value_counts()[1]  # Calculating scale_pos_weight for handling class imbalance

    model_pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('classifier', xgb.XGBClassifier(
            objective='binary:logistic',
            eval_metric='logloss',
            scale_pos_weight=scale_pos_weight,
            use_label_encoder=False,
            random_state=42
        ))
    ])

    print("Training the XGBoost model...")
    model_pipeline.fit(X_train, y_train)
    print("Model training complete.")

"""**Model Evaluation**

1. Classification Report
"""

if 'model_pipeline' in locals() and df is not None:
    print("\n--- Model Evaluation ---")
    y_pred = model_pipeline.predict(X_test)
    y_pred_proba = model_pipeline.predict_proba(X_test)[:, 1]

    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, target_names=['Non-Fraud', 'Fraud']))

"""2. Confusion Matrix"""

print("\nConfusion Matrix:")
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Non-Fraud', 'Fraud'],
            yticklabels=['Non-Fraud', 'Fraud'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

"""3. ROC-AUC Score with ROC-AUC Curve"""

roc_auc = roc_auc_score(y_test, y_pred_proba)
print(f"\nROC-AUC Score: {roc_auc:.4f}")

fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

"""4. Precision-Recall Curve"""

avg_precision = average_precision_score(y_test, y_pred_proba)
print(f"\nAverage Precision-Recall Score: {avg_precision:.4f}")
precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)
plt.figure()
plt.step(recall, precision, where='post', label=f'AP={avg_precision:.2f}')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.ylim([0.0, 1.05])
plt.xlim([0.0, 1.0])
plt.title('Precision-Recall Curve')
plt.legend(loc="upper right")
plt.show()